# ğŸ“Š StockSeer â€“ PredicciÃ³n bursÃ¡til basada en noticias

## ğŸ“Œ DescripciÃ³n del proyecto y propuesta de valor

**StockSeer** es una herramienta diseÃ±ada para ayudar a los usuarios a tomar decisiones de inversiÃ³n informadas mediante el anÃ¡lisis conjunto de datos financieros y noticias del mercado. El sistema recopila:

* ğŸ“° Noticias econÃ³micas mediante la API de [NewsAPI.org](https://newsapi.org/).
* ğŸ’¹ Datos bursÃ¡tiles (intradiarios e histÃ³ricos) a travÃ©s de [Alpha Vantage](https://www.alphavantage.co).

El foco principal es la acciÃ³n de Amazon (AMZN). A travÃ©s de tÃ©cnicas de anÃ¡lisis de sentimiento sobre el contenido informativo, y una estructura de datos organizada en un datamart, se genera una predicciÃ³n visual en un dashboard interactivo.

ğŸ¯ **Propuesta de valor:**
Proporcionar un soporte a la toma de decisiones, integrando un contexto informativo y el comportamiento de mercado en una Ãºnica herramienta.

---

## ğŸ”Œ ElecciÃ³n de APIs

Se ha optado por las siguientes APIs:

* **[NewsAPI.org](https://newsapi.org/):** Proporciona noticias econÃ³micas filtradas por palabras clave. En nuestro caso, se han utilizado tÃ©rminos relacionados con *Amazon* para obtener titulares relevantes que se analizan mediante tÃ©cnicas de NLP.
* **[Alpha Vantage](https://www.alphavantage.co):** Ofrece datos bursÃ¡tiles fiables, tanto intradÃ­a como histÃ³ricos.

âœ… Ambas APIs:

* Disponen de planes gratuitos.
* Cuentan con documentaciÃ³n clara.
* Se integran fÃ¡cilmente en pipelines automÃ¡ticos.

ğŸ§  La elecciÃ³n estÃ¡ basada en la **complementariedad entre datos estructurados** (precios de apertura, cierre, volumen) y **datos no estructurados** (titulares, noticias), lo que enriquece los modelos predictivos y mejora la calidad de la decisiÃ³n.

---

## ğŸ”§ Requisitos

### ğŸ“¦ Dependencias generales

* Tener instalado y en ejecuciÃ³n el broker de mensajerÃ­a **ActiveMQ**.
* Tener **JDK 21** correctamente configurado.
* Tener **Python 3.11.9 o superior** en el PATH del sistema.

### ğŸ LibrerÃ­as necesarias en el entorno Python

Para ejecutar los scripts de anÃ¡lisis y el dashboard, asegÃºrate de tener instaladas las siguientes dependencias:

```python
import sys
import os
import argparse
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import sqlite3
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, TimeSeriesSplit, RandomizedSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor, StackingRegressor
from sklearn.linear_model import ElasticNet, LinearRegression
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from scipy.stats import randint, uniform
import joblib
import streamlit as st
import plotly.graph_objects as go
```

---

## ğŸ§© Estructura del sistema

Este proyecto sigue una combinaciÃ³n de **Clean Architecture** y **Arquitectura Hexagonal (Ports & Adapters)**. Aunque los mÃ³dulos varÃ­an ligeramente segÃºn su propÃ³sito, mantienen una estructura coherente, desacoplada y extensible.

Esta organizaciÃ³n es especialmente evidente en los **mÃ³dulos tipo *feeder***, donde el diseÃ±o modular y los principios SOLID se aplican de forma rigurosa para separar la lÃ³gica de negocio de las implementaciones concretas (como fuentes de datos o mecanismos de almacenamiento).

### ğŸ”„ Capas principales:

* **Modelo (dominio):** Contiene las entidades puras del sistema (por ejemplo, `AlphaVantageEvent`), independientes de frameworks o librerÃ­as externas.
* **Puertos (interfaces):** Definen las necesidades del sistema (entrada y salida), como `OpeningClosingEventSaver` o `IntradayStockEventFetcher`.
* **Adaptadores (infrastructure.adapters):** Implementan los puertos con tecnologÃ­as concretas (APIs, bases de datos, brokers de mensajerÃ­a). Ejemplos: `SqliteEventSaver`, `ActivemqPublisher`, `AlphaVantageIntradayFetcher`.
* **Controlador:** Orquesta el flujo entre proveedores y almacenamiento, como `IntradayFetcher`, encapsulando el caso de uso principal del feeder.
* **Utils:** Contiene clases reutilizables para tareas transversales (como `DateParser`, `MarketCloseScheduler` o `TimestampParser`).

ğŸ“Œ Esta estructura garantiza que **las dependencias fluyen hacia el dominio**, no al revÃ©s. El dominio y los puertos no conocen detalles tÃ©cnicos concretos, lo que facilita testeo, mantenimiento y evoluciÃ³n tecnolÃ³gica sin afectar la lÃ³gica de negocio.

---

### ğŸ§  Principios de diseÃ±o aplicados

A lo largo del desarrollo, especialmente en los *feeders*, se han seguido los siguientes principios:

* **SRP** (Single Responsibility Principle): cada clase tiene una Ãºnica responsabilidad clara (por ejemplo, `SqliteManager` solo gestiona la persistencia en SQLite).
* **OCP** (Open/Closed Principle): puedes aÃ±adir nuevos almacenamientos o proveedores sin modificar el controlador (`IntradayFetcher`).
* **DRY** (Donâ€™t Repeat Yourself): se ha evitado duplicar lÃ³gica, centralizando el parseo, almacenamiento y mapeo en clases dedicadas.
* **YAGNI** (You Arenâ€™t Gonna Need It): no se ha incluido cÃ³digo innecesario ni dependencias superfluas.

---

### ğŸ§ª Ejemplo concreto

```java
OpeningClosingEventSaver storage = new ActivemqPublisher(
    argsValues.get("BROKER_URL"),
    argsValues.get("TOPIC_NAME"),
    DateParser.parse(argsValues.get("TODAY"))
);

OpeningClosingEventSaver storage = new SqliteManager(
    argsValues.get("DB_URL"),
    DateParser.parse(argsValues.get("TODAY"))
);

IntradayStockEventFetcher provider = new AlphaVantageIntradayFetcher(
    argsValues.get("API_KEY")
);

IntradayFetcher fetcher = new IntradayFetcher(
    argsValues.get("SYMBOL"),
    provider,
    storage,
    argsValues.get("DB_URL"),
    TimestampParser.parseMarketClose(argsValues.get("MARKET_CLOSE"))
);
```

ğŸ‘‰ Esto demuestra la flexibilidad y extensibilidad del sistema, sin alterar la lÃ³gica de negocio al cambiar implementaciones concretas.

---

## ğŸ§± MÃ³dulos del proyecto

A continuaciÃ³n se detallan los distintos mÃ³dulos que componen el sistema, junto con su diagrama de clases correspondiente.
Haz clic en el nombre de cada imagen para visualizarla en una nueva pestaÃ±a.

---

### ğŸ“¦ `time-series-intraday-feeder`

<details>
  <summary>ğŸ“„ Ver diagrama de clases</summary>

ğŸ”— [Abrir imagen en el navegador](./diagrams/time-series-intraday-feeder-class-diagram.png)

</details>

Este mÃ³dulo se encarga de:

* Obtener datos bursÃ¡tiles intradÃ­a de la API de **AlphaVantage**, centrados en el sÃ­mbolo `AMZN`.
* Filtrar los eventos correspondientes al **inicio y cierre exacto del mercado estadounidense** (09:30 y 16:00 en Nueva York).
* Publicar los eventos en un **broker ActiveMQ** o almacenarlos en **SQLite**, dependiendo de la configuraciÃ³n proporcionada en `args.txt`.

### ğŸ“ Flujo simplificado

1. `IntradayFetcher` espera al cierre del mercado.
2. Llama a `AlphaVantageIntradayFetcher.fetch()`.
3. Filtra apertura/cierre exactos mediante `MarketHoursFilter`.
4. Guarda en `SQLite` o publica en `ActiveMQ`.

### ğŸ§© Principios y patrones aplicados

* **Clean Architecture + Hexagonal**: separaciÃ³n clara de puertos (`IntradayStockEventFetcher`, `OpeningClosingEventSaver`) y adaptadores (`AlphaVantageIntradayFetcher`, `SqliteManager`).
* **Factory Pattern**: construcciÃ³n de eventos con `AlphaVantageEventFactory`.
* **Strategy Pattern**: selecciÃ³n dinÃ¡mica del almacenamiento segÃºn configuraciÃ³n.
* **SRP / OCP / DRY** aplicados rigurosamente en clases como `IntradayFetcher`, `MarketCloseScheduler`, `SqliteManager`.

---

### ğŸ—ï¸ `news-api-feeder`

<details>
  <summary>ğŸ“„ Ver diagrama de clases</summary>

ğŸ”— [Abrir imagen en el navegador](./diagrams/news-api-feeder-class-diagram.png)

</details>

Este mÃ³dulo se encarga de:

* Recuperar **noticias econÃ³micas** mediante la API de [NewsAPI.org](https://newsapi.org/), filtradas por tema y fecha.
* Crear eventos del tipo `ArticleEvent` que contienen tÃ­tulo, contenido, fecha de publicaciÃ³n, etc.
* Enriquecer el contenido mediante tÃ©cnicas de scraping.
* **Publicar** los artÃ­culos procesados en un broker (`ActiveMQ`) o almacenarlos en una base de datos local SQLite.

### ğŸ“ Flujo simplificado

1. `ArticleController` calcula el rango de fechas del dÃ­a anterior.
2. Solicita los artÃ­culos usando `NewsApiFetcher`.
3. Procesa y enriquece cada artÃ­culo (`ArticleProcessor`, `ArticleEnricher`).
4. Almacena en SQLite o publica en cola/tÃ³pico con ActiveMQ.

### ğŸ§© Principios y patrones aplicados

* **Clean Architecture**: uso de puertos (`ArticleEventFetcher`, `ArticleSaver`) e interfaces desacopladas.
* **Adapter Pattern**: `NewsApiFetcher`, `DatabaseManager` y `ArticleEventPublisher` implementan las interfaces de persistencia y captura.
* **SRP y OCP**: cada clase tiene una responsabilidad clara y puede ampliarse fÃ¡cilmente (aÃ±adir otro API de noticias, por ejemplo).
* 
---

### ğŸ—ƒï¸ `event-store-builder`

<details>
  <summary>ğŸ“„ Ver diagrama de clases</summary>

ğŸ”— [Abrir imagen en el navegador](./diagrams/event-store-builder-class-diagram.png)

</details>

Este mÃ³dulo actÃºa como **consumidor durable** de eventos publicados en el broker **ActiveMQ**. Su objetivo es escuchar eventos de distintos tÃ³picos, deserializarlos, extraer metainformaciÃ³n clave (`ts`, `ss`, `topic`) y almacenarlos en el sistema de ficheros con una estructura organizada.

### ğŸ§  Principales caracterÃ­sticas

* Se suscribe de forma **durable** a un tÃ³pico mediante `ActiveMQSubscriber`, asegurando la entrega incluso tras reinicios.
* Cada mensaje JSON recibido se procesa mediante el controlador `EventHandler`, que lo transforma en un objeto `Event`.
* Los eventos se guardan en ficheros `.events` con la siguiente estructura:

```
eventstore/{topic}/{ss}/{YYYYMMDD}.events
```

Cada lÃ­nea del fichero contiene un evento en formato JSON.

* Utiliza el patrÃ³n **Hexagonal** con:

  * **Puerto**: `EventStorage`
  * **Adaptador**: `FileSystemStorage`, que gestiona la persistencia fÃ­sica.

### ğŸ“ Flujo simplificado

1. `ActiveMQSubscriber` recibe un mensaje del tÃ³pico.
2. `EventHandler` deserializa el mensaje y crea un `Event`.
3. El evento se guarda con `FileSystemStorage`, creando la ruta si no existe.

### ğŸ§© Principios y patrones aplicados

* **Clean Architecture**: separaciÃ³n entre infraestructura (`ActiveMQSubscriber`, `FileSystemStorage`) y lÃ³gica de control (`EventHandler`).
* **Adapter Pattern**: `FileSystemStorage` implementa la interfaz `EventStorage`.
* **SRP / OCP**: modularidad completa entre suscripciÃ³n, transformaciÃ³n y almacenamiento.

---

## ğŸ› ï¸ Instrucciones para compilar y ejecutar cada mÃ³dulo

Todos los mÃ³dulos del sistema estÃ¡n desarrollados en **Java 21** usando **Maven**. Algunos de ellos tambiÃ©n invocan scripts externos en **Python 3.11+** para realizar tareas auxiliares como el enriquecimiento de contenido.

---

### ğŸ§¾ ConfiguraciÃ³n de ejecuciÃ³n

Cada mÃ³dulo necesita un archivo `args.txt` con sus parÃ¡metros de configuraciÃ³n (como claves API, URLs de bases de datos o topics).
Este archivo debe proporcionarse al ejecutar el mÃ³dulo.

A continuaciÃ³n, se muestra un ejemplo de configuraciÃ³n por mÃ³dulo:

---

#### ğŸ“¦ `time-series-intraday-feeder` â€“ `args.txt`

```txt
API_KEY=TuAPIKEY
DB_URL=jdbc:sqlite:data.db
SYMBOL=AMZN
FETCH_INTERVAL_MINUTES=1
STORAGE_MODE=activemq o sqlite
BROKER_URL=tcp://localhost:61616
TOPIC_NAME=StockQuotes
TODAY=fecha de hoy con el siguiente formato 2025-06-25
MARKET_CLOSE= En caso de que lo quieras ejecutar en una hora que no sea el cierre del mercado, deberÃ¡s poner la hora actual de nueva york
```

---

#### ğŸ—ï¸ `news-api-feeder` â€“ `args.txt`

```txt
DB_URL=jdbc:sqlite:ruta
API_KEY=TuAPIKEY
DEFAULT_LANGUAGE=en
FETCH_INTERVAL_HOURS=1
BROKER_URL=tcp://localhost:61616
QUEUE_NAME=Articles
TOPIC_NAME=Articles
STORAGE_TARGET=broker
SOURCE_SYSTEM=NewsApiFeeder
QUERY=AMZN
```

---

#### ğŸ—ƒï¸ `event-store-builder` â€“ `args.txt`

```txt
BROKER_URL=tcp://localhost:61616
TOPICS=Articles, StockQuotes
CLIENT_ID=event-store-builder-client
```

---

### âš™ï¸ Formas de ejecuciÃ³n

Existen dos formas principales de ejecutar los mÃ³dulos:

* ğŸ§ª **OpciÃ³n 1 (vÃ¡lida pero menos cÃ³moda):** ejecutar el `.jar` desde la terminal especificando la ruta del archivo `args.txt`.
* âœ… **OpciÃ³n recomendada:** aÃ±adir la ruta al `args.txt` directamente en la **configuraciÃ³n de arranque del mÃ©todo `main()`** desde el entorno de desarrollo de IntelliJ IDEA.

Esto permite lanzar los mÃ³dulos con un solo clic y la ejecuciÃ³n ordenada de los mÃ³dulos.

ğŸ“· A continuaciÃ³n se muestra un ejemplo visual de esta configuraciÃ³n:

> ![Ejemplo configuraciÃ³n Main](./images/configuracion-main-ejecucion.png)

NOTA: En caso de necesitar el entorno de python, observar como en la variable de entorno hay que pone PYTHON_EXECUTABLE=ruta_del_entorno
---

### â±ï¸ Orden de ejecuciÃ³n de los mÃ³dulos

Para que el sistema funcione correctamente, se recomienda ejecutar los mÃ³dulos en el siguiente orden:

1. **`event-store-builder`**
   (Empieza escuchando en el broker y estÃ¡ listo para almacenar eventos que lleguen)
   Para ejecutar este mÃ³dulo proporcionamos ya la carpeta evenstore dentro de su mÃ³dulo correspondiente, con el histÃ³rico de cada api, porque sino el tiempo de     ejecuciÃ³n serÃ­a muy largo.

3. **`time-series-intraday-feeder`**
   (Obtiene y publica datos bursÃ¡tiles de apertura/cierre)

4. **`news-api-feeder`**
   (Recupera noticias y publica o guarda los artÃ­culos enriquecidos)

De este modo, garantizas que todos los consumidores estÃ©n activos antes de que se publiquen los eventos.

---

Perfecto, aquÃ­ tienes un texto redactado para el README o memoria del proyecto. Incluye dos huecos claros para insertar imÃ¡genes: uno para el esquema del `StackingRegressor` y otro para una muestra del `CSV` de entrada. EstÃ¡ redactado de forma profesional, pero comprensible:

---

## ğŸ¤– Entrenamiento del modelo de predicciÃ³n

El entrenamiento del modelo predictivo se ha llevado a cabo a partir de los datos almacenados en la tabla `clean_datamart`, ubicada en la base de datos SQLite generada por el mÃ³dulo de integraciÃ³n de eventos. Esta tabla contiene informaciÃ³n relevante sobre el mercado bursÃ¡til y noticias procesadas, ya tratadas y enriquecidas previamente.

Dicho entrenamiento sigue una estrategia de *stacking*, donde se combinan diversos modelos de regresiÃ³n con el objetivo de mejorar la capacidad predictiva. Los modelos base utilizados incluyen:

* `RandomForestRegressor`
* `SVR` (Support Vector Regressor)
* `ElasticNet`
* `DecisionTreeRegressor`
* `KNeighborsRegressor`

AdemÃ¡s, para aquellos modelos que lo requieren, se ha aplicado escalado de caracterÃ­sticas mediante `StandardScaler` encapsulado en un `Pipeline`.

El modelo final es un `StackingRegressor` que integra a todos los anteriores y utiliza un `RandomForestRegressor` como estimador final. A continuaciÃ³n se muestra el esquema representativo de la arquitectura del `StackingRegressor`:

ğŸ“Œ **\[Inserta aquÃ­ una imagen/diagrama del StackingRegressor]**

### ğŸ§ª ValidaciÃ³n y mÃ©tricas

Para validar el rendimiento del modelo, se ha empleado una estrategia de validaciÃ³n cruzada basada en series temporales (`TimeSeriesSplit`), evitando asÃ­ el uso de datos futuros para predecir el pasado. Tras realizar una bÃºsqueda aleatoria de hiperparÃ¡metros (`RandomizedSearchCV`) sobre cada estimador base, se ha obtenido un error cuadrÃ¡tico medio (RMSE) competitivo sobre el conjunto de test, lo que indica una buena capacidad de generalizaciÃ³n del modelo entrenado.

### ğŸ§¾ IngenierÃ­a de caracterÃ­sticas

Antes del entrenamiento, se han generado nuevas variables derivadas con el objetivo de capturar dinÃ¡micas relevantes del mercado. Entre estas se incluyen:

* Diferencias temporales (`delta_open`, `delta_close`, `delta_sent`)
* Rango diario (`range`)
* Volatilidad reciente (`volatility`)
* Momentum a corto plazo (`momentum`)
* Variable objetivo: precio de apertura del dÃ­a siguiente (`y`)

### ğŸ§® Datos utilizados

La tabla `clean_datamart` se exporta automÃ¡ticamente a un fichero CSV que sirve como entrada directa al pipeline de entrenamiento. La siguiente imagen muestra un extracto representativo del conjunto de datos empleados:

ğŸ“Œ **\[Inserta aquÃ­ una imagen de una muestra del CSV generado]**

---
