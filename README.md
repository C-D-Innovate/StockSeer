# üìä StockSeer ‚Äì Predicci√≥n burs√°til basada en noticias

## üìå Descripci√≥n del proyecto y propuesta de valor

**StockSeer** es una herramienta dise√±ada para ayudar a los usuarios a tomar decisiones de inversi√≥n informadas mediante el an√°lisis conjunto de datos financieros y noticias del mercado. El sistema recopila:

* üì∞ Noticias econ√≥micas mediante la API de [NewsAPI.org](https://newsapi.org/).
* üíπ Datos burs√°tiles (intradiarios e hist√≥ricos) a trav√©s de [Alpha Vantage](https://www.alphavantage.co).

El foco principal es la acci√≥n de Amazon (AMZN). A trav√©s de t√©cnicas de an√°lisis de sentimiento sobre el contenido informativo, y una estructura de datos organizada en un datamart, se genera una predicci√≥n visual en un dashboard interactivo.

üéØ **Propuesta de valor:**
Proporcionar un soporte a la toma de decisiones, integrando un contexto informativo y el comportamiento de mercado en una √∫nica herramienta.

---

## üîå Elecci√≥n de APIs

Se ha optado por las siguientes APIs:

* **[NewsAPI.org](https://newsapi.org/):** Proporciona noticias econ√≥micas filtradas por palabras clave. En nuestro caso, se han utilizado t√©rminos relacionados con *Amazon* para obtener titulares relevantes que se analizan mediante t√©cnicas de NLP.
* **[Alpha Vantage](https://www.alphavantage.co):** Ofrece datos burs√°tiles fiables, tanto intrad√≠a como hist√≥ricos.

‚úÖ Ambas APIs:

* Disponen de planes gratuitos.
* Cuentan con documentaci√≥n clara.
* Se integran f√°cilmente en pipelines autom√°ticos.

üß† La elecci√≥n est√° basada en la **complementariedad entre datos estructurados** (precios de apertura, cierre, volumen) y **datos no estructurados** (titulares, noticias), lo que enriquece los modelos predictivos y mejora la calidad de la decisi√≥n.

---

## üîß Requisitos

### üì¶ Dependencias generales

* Tener instalado y en ejecuci√≥n el broker de mensajer√≠a **ActiveMQ**.
* Tener **JDK 21** correctamente configurado.
* Tener **Python 3.11.9 o superior** en el PATH del sistema.

### üêç Librer√≠as necesarias en el entorno Python

Para ejecutar los scripts de an√°lisis y el dashboard, aseg√∫rate de tener instaladas las siguientes dependencias:

```python
import sys
import os
import argparse
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import sqlite3
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, TimeSeriesSplit, RandomizedSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor, StackingRegressor
from sklearn.linear_model import ElasticNet, LinearRegression
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from scipy.stats import randint, uniform
import joblib
import streamlit as st
import plotly.graph_objects as go
```

---

## üß© Estructura del sistema

Este proyecto sigue una combinaci√≥n de **Clean Architecture** y **Arquitectura Hexagonal (Ports & Adapters)**. Aunque los m√≥dulos var√≠an ligeramente seg√∫n su prop√≥sito, mantienen una estructura coherente, desacoplada y extensible.

Esta organizaci√≥n es especialmente evidente en los **m√≥dulos tipo *feeder***, donde el dise√±o modular y los principios SOLID se aplican de forma rigurosa para separar la l√≥gica de negocio de las implementaciones concretas (como fuentes de datos o mecanismos de almacenamiento).

### üîÑ Capas principales:

* **Modelo (dominio):** Contiene las entidades puras del sistema (por ejemplo, `AlphaVantageEvent`), independientes de frameworks o librer√≠as externas.
* **Puertos (interfaces):** Definen las necesidades del sistema (entrada y salida), como `OpeningClosingEventSaver` o `IntradayStockEventFetcher`.
* **Adaptadores (infrastructure.adapters):** Implementan los puertos con tecnolog√≠as concretas (APIs, bases de datos, brokers de mensajer√≠a). Ejemplos: `SqliteEventSaver`, `ActivemqPublisher`, `AlphaVantageIntradayFetcher`.
* **Controlador:** Orquesta el flujo entre proveedores y almacenamiento, como `IntradayFetcher`, encapsulando el caso de uso principal del feeder.
* **Utils:** Contiene clases reutilizables para tareas transversales (como `DateParser`, `MarketCloseScheduler` o `TimestampParser`).

üìå Esta estructura garantiza que **las dependencias fluyen hacia el dominio**, no al rev√©s. El dominio y los puertos no conocen detalles t√©cnicos concretos, lo que facilita testeo, mantenimiento y evoluci√≥n tecnol√≥gica sin afectar la l√≥gica de negocio.

---

### üß† Principios de dise√±o aplicados

A lo largo del desarrollo, especialmente en los *feeders*, se han seguido los siguientes principios:

* **SRP** (Single Responsibility Principle): cada clase tiene una √∫nica responsabilidad clara (por ejemplo, `SqliteManager` solo gestiona la persistencia en SQLite).
* **OCP** (Open/Closed Principle): puedes a√±adir nuevos almacenamientos o proveedores sin modificar el controlador (`IntradayFetcher`).
* **DRY** (Don‚Äôt Repeat Yourself): se ha evitado duplicar l√≥gica, centralizando el parseo, almacenamiento y mapeo en clases dedicadas.
* **YAGNI** (You Aren‚Äôt Gonna Need It): no se ha incluido c√≥digo innecesario ni dependencias superfluas.

---

### üß™ Ejemplo concreto

```java
OpeningClosingEventSaver storage = new ActivemqPublisher(
    argsValues.get("BROKER_URL"),
    argsValues.get("TOPIC_NAME"),
    DateParser.parse(argsValues.get("TODAY"))
);

OpeningClosingEventSaver storage = new SqliteManager(
    argsValues.get("DB_URL"),
    DateParser.parse(argsValues.get("TODAY"))
);

IntradayStockEventFetcher provider = new AlphaVantageIntradayFetcher(
    argsValues.get("API_KEY")
);

IntradayFetcher fetcher = new IntradayFetcher(
    argsValues.get("SYMBOL"),
    provider,
    storage,
    argsValues.get("DB_URL"),
    TimestampParser.parseMarketClose(argsValues.get("MARKET_CLOSE"))
);
```

üëâ Esto demuestra la flexibilidad y extensibilidad del sistema, sin alterar la l√≥gica de negocio al cambiar implementaciones concretas.

---

## üß± M√≥dulos del proyecto

A continuaci√≥n se detallan los distintos m√≥dulos que componen el sistema, junto con su diagrama de clases correspondiente.
Haz clic en el nombre de cada imagen para visualizarla en una nueva pesta√±a.

---

### üì¶ `time-series-intraday-feeder`

<details>
  <summary>üìÑ Ver diagrama de clases</summary>

üîó [Abrir imagen en el navegador](./diagrams/time-series-intraday-feeder-class-diagram.png)

</details>

Este m√≥dulo se encarga de:

* Obtener datos burs√°tiles intrad√≠a de la API de **AlphaVantage**, centrados en el s√≠mbolo `AMZN`.
* Filtrar los eventos correspondientes al **inicio y cierre exacto del mercado estadounidense** (09:30 y 16:00 en Nueva York).
* Publicar los eventos en un **broker ActiveMQ** o almacenarlos en **SQLite**, dependiendo de la configuraci√≥n proporcionada en `args.txt`.

### üìÅ Flujo simplificado

1. `IntradayFetcher` espera al cierre del mercado.
2. Llama a `AlphaVantageIntradayFetcher.fetch()`.
3. Filtra apertura/cierre exactos mediante `MarketHoursFilter`.
4. Guarda en `SQLite` o publica en `ActiveMQ`.

### üß© Principios y patrones aplicados

* **Clean Architecture + Hexagonal**: separaci√≥n clara de puertos (`IntradayStockEventFetcher`, `OpeningClosingEventSaver`) y adaptadores (`AlphaVantageIntradayFetcher`, `SqliteManager`).
* **Factory Pattern**: construcci√≥n de eventos con `AlphaVantageEventFactory`.
* **Strategy Pattern**: selecci√≥n din√°mica del almacenamiento seg√∫n configuraci√≥n.
* **SRP / OCP / DRY** aplicados rigurosamente en clases como `IntradayFetcher`, `MarketCloseScheduler`, `SqliteManager`.

---

### üóûÔ∏è `news-api-feeder`

<details>
  <summary>üìÑ Ver diagrama de clases</summary>

üîó [Abrir imagen en el navegador](./diagrams/news-api-feeder-class-diagram.png)

</details>

Este m√≥dulo se encarga de:

* Recuperar **noticias econ√≥micas** mediante la API de [NewsAPI.org](https://newsapi.org/), filtradas por tema y fecha.
* Crear eventos del tipo `ArticleEvent` que contienen t√≠tulo, contenido, fecha de publicaci√≥n, etc.
* Enriquecer el contenido mediante t√©cnicas de scraping.
* **Publicar** los art√≠culos procesados en un broker (`ActiveMQ`) o almacenarlos en una base de datos local SQLite.

### üìÅ Flujo simplificado

1. `ArticleController` calcula el rango de fechas del d√≠a anterior.
2. Solicita los art√≠culos usando `NewsApiFetcher`.
3. Procesa y enriquece cada art√≠culo (`ArticleProcessor`, `ArticleEnricher`).
4. Almacena en SQLite o publica en cola/t√≥pico con ActiveMQ.

### üß© Principios y patrones aplicados

* **Clean Architecture**: uso de puertos (`ArticleEventFetcher`, `ArticleSaver`) e interfaces desacopladas.
* **Adapter Pattern**: `NewsApiFetcher`, `DatabaseManager` y `ArticleEventPublisher` implementan las interfaces de persistencia y captura.
* **SRP y OCP**: cada clase tiene una responsabilidad clara y puede ampliarse f√°cilmente (a√±adir otro API de noticias, por ejemplo).
* 
---

### üóÉÔ∏è `event-store-builder`

<details>
  <summary>üìÑ Ver diagrama de clases</summary>

üîó [Abrir imagen en el navegador](./diagrams/event-store-builder-class-diagram.png)

</details>

Este m√≥dulo act√∫a como **consumidor durable** de eventos publicados en el broker **ActiveMQ**. Su objetivo es escuchar eventos de distintos t√≥picos, deserializarlos, extraer metainformaci√≥n clave (`ts`, `ss`, `topic`) y almacenarlos en el sistema de ficheros con una estructura organizada.

### üß† Principales caracter√≠sticas

* Se suscribe de forma **durable** a un t√≥pico mediante `ActiveMQSubscriber`, asegurando la entrega incluso tras reinicios.
* Cada mensaje JSON recibido se procesa mediante el controlador `EventHandler`, que lo transforma en un objeto `Event`.
* Los eventos se guardan en ficheros `.events` con la siguiente estructura:

```
eventstore/{topic}/{ss}/{YYYYMMDD}.events
```

Cada l√≠nea del fichero contiene un evento en formato JSON.

* Utiliza el patr√≥n **Hexagonal** con:

  * **Puerto**: `EventStorage`
  * **Adaptador**: `FileSystemStorage`, que gestiona la persistencia f√≠sica.

### üìÅ Flujo simplificado

1. `ActiveMQSubscriber` recibe un mensaje del t√≥pico.
2. `EventHandler` deserializa el mensaje y crea un `Event`.
3. El evento se guarda con `FileSystemStorage`, creando la ruta si no existe.

### üß© Principios y patrones aplicados

* **Clean Architecture**: separaci√≥n entre infraestructura (`ActiveMQSubscriber`, `FileSystemStorage`) y l√≥gica de control (`EventHandler`).
* **Adapter Pattern**: `FileSystemStorage` implementa la interfaz `EventStorage`.
* **SRP / OCP**: modularidad completa entre suscripci√≥n, transformaci√≥n y almacenamiento.

---

## üõ†Ô∏è Instrucciones para compilar y ejecutar cada m√≥dulo

Todos los m√≥dulos del sistema est√°n desarrollados en **Java 21** usando **Maven**. Algunos de ellos tambi√©n invocan scripts externos en **Python 3.11+** para realizar tareas auxiliares como el enriquecimiento de contenido.

---

### üßæ Configuraci√≥n de ejecuci√≥n

Cada m√≥dulo necesita un archivo `args.txt` con sus par√°metros de configuraci√≥n (como claves API, URLs de bases de datos o topics).
Este archivo debe proporcionarse al ejecutar el m√≥dulo.

A continuaci√≥n, se muestra un ejemplo de configuraci√≥n por m√≥dulo:

---

#### üì¶ `time-series-intraday-feeder` ‚Äì `args.txt`

```txt
API_KEY=TuAPIKEY
DB_URL=jdbc:sqlite:data.db
SYMBOL=AMZN
FETCH_INTERVAL_MINUTES=1
STORAGE_MODE=activemq o sqlite
BROKER_URL=tcp://localhost:61616
TOPIC_NAME=StockQuotes
TODAY=fecha de hoy con el siguiente formato 2025-06-25
MARKET_CLOSE= En caso de que lo quieras ejecutar en una hora que no sea el cierre del mercado, deber√°s poner la hora actual de nueva york
```

---

#### üóûÔ∏è `news-api-feeder` ‚Äì `args.txt`

```txt
DB_URL=jdbc:sqlite:ruta
API_KEY=TuAPIKEY
DEFAULT_LANGUAGE=en
FETCH_INTERVAL_HOURS=1
BROKER_URL=tcp://localhost:61616
QUEUE_NAME=Articles
TOPIC_NAME=Articles
STORAGE_TARGET=broker
SOURCE_SYSTEM=NewsApiFeeder
QUERY=AMZN
```

---

#### üóÉÔ∏è `event-store-builder` ‚Äì `args.txt`

```txt
BROKER_URL=tcp://localhost:61616
TOPICS=Articles, StockQuotes
CLIENT_ID=event-store-builder-client
```

---

### ‚öôÔ∏è Formas de ejecuci√≥n

Existen dos formas principales de ejecutar los m√≥dulos:

* üß™ **Opci√≥n 1 (v√°lida pero menos c√≥moda):** ejecutar el `.jar` desde la terminal especificando la ruta del archivo `args.txt`.
* ‚úÖ **Opci√≥n recomendada:** a√±adir la ruta al `args.txt` directamente en la **configuraci√≥n de arranque del m√©todo `main()`** desde el entorno de desarrollo de IntelliJ IDEA.

Esto permite lanzar los m√≥dulos con un solo clic y la ejecuci√≥n ordenada de los m√≥dulos.

üì∑ A continuaci√≥n se muestra un ejemplo visual de esta configuraci√≥n:

![Ejemplo configuraci√≥n Main](fotos_readme/configuracion_main.png)

NOTA: En caso de necesitar el entorno de python, observar como en la variable de entorno hay que pone PYTHON_EXECUTABLE=ruta_del_entorno
---

### ‚è±Ô∏è Orden de ejecuci√≥n de los m√≥dulos

Para que el sistema funcione correctamente, se recomienda ejecutar los m√≥dulos en el siguiente orden:

1. **`event-store-builder`**
   (Empieza escuchando en el broker y est√° listo para almacenar eventos que lleguen)
   Para ejecutar este m√≥dulo proporcionamos ya la carpeta evenstore dentro de su m√≥dulo correspondiente, con el hist√≥rico de cada api, porque sino el tiempo de     ejecuci√≥n ser√≠a muy largo.

3. **`time-series-intraday-feeder`**
   (Obtiene y publica datos burs√°tiles de apertura/cierre)

4. **`news-api-feeder`**
   (Recupera noticias y publica o guarda los art√≠culos enriquecidos)

De este modo, garantizas que todos los consumidores est√©n activos antes de que se publiquen los eventos.

---

Perfecto, aqu√≠ tienes un texto redactado para el README o memoria del proyecto. Incluye dos huecos claros para insertar im√°genes: uno para el esquema del `StackingRegressor` y otro para una muestra del `CSV` de entrada. Est√° redactado de forma profesional, pero comprensible:

---

## ü§ñ Entrenamiento del modelo de predicci√≥n

El entrenamiento del modelo predictivo se ha llevado a cabo a partir de los datos almacenados en la tabla `clean_datamart`, ubicada en la base de datos SQLite generada por el m√≥dulo de integraci√≥n de eventos. Esta tabla contiene informaci√≥n relevante sobre el mercado burs√°til y noticias procesadas, ya tratadas y enriquecidas previamente.

Dicho entrenamiento sigue una estrategia de *stacking*, donde se combinan diversos modelos de regresi√≥n con el objetivo de mejorar la capacidad predictiva. Los modelos base utilizados incluyen:

* `RandomForestRegressor`
* `SVR` (Support Vector Regressor)
* `ElasticNet`
* `DecisionTreeRegressor`
* `KNeighborsRegressor`

Adem√°s, para aquellos modelos que lo requieren, se ha aplicado escalado de caracter√≠sticas mediante `StandardScaler` encapsulado en un `Pipeline`.

El modelo final es un `StackingRegressor` que integra a todos los anteriores y utiliza un `RandomForestRegressor` como estimador final. A continuaci√≥n se muestra el esquema representativo de la arquitectura del `StackingRegressor`:

![Esquema](fotos_readme/Esquema_regressor.png)

### üß™ Validaci√≥n y m√©tricas

Para validar el rendimiento del modelo, se ha empleado una estrategia de validaci√≥n cruzada basada en series temporales (`TimeSeriesSplit`), evitando as√≠ el uso de datos futuros para predecir el pasado. Tras realizar una b√∫squeda aleatoria de hiperpar√°metros (`RandomizedSearchCV`) sobre cada estimador base, se ha obtenido un error cuadr√°tico medio (RMSE) competitivo sobre el conjunto de test, lo que indica una buena capacidad de generalizaci√≥n del modelo entrenado.

### üßæ Ingenier√≠a de caracter√≠sticas

Antes del entrenamiento, se han generado nuevas variables derivadas con el objetivo de capturar din√°micas relevantes del mercado. Entre estas se incluyen:

* Diferencias temporales (`delta_open`, `delta_close`, `delta_sent`)
* Rango diario (`range`)
* Volatilidad reciente (`volatility`)
* Momentum a corto plazo (`momentum`)
* Variable objetivo: precio de apertura del d√≠a siguiente (`y`)

### üßÆ Datos utilizados

La tabla `clean_datamart` se exporta autom√°ticamente a un fichero CSV que sirve como entrada directa al pipeline de entrenamiento. La siguiente imagen muestra un extracto representativo del conjunto de datos empleados:

![Datos de entrenamiento](fotos_readme/ejemplo_csv.png)

---
